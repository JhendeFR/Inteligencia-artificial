{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresion lineal multiple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizado para manejos de directorios y rutas\n",
    "import os\n",
    "\n",
    "# Computacion vectorial y cientifica para python\n",
    "import numpy as np\n",
    "\n",
    "#Importacion de pandas para cargar el DataSet\n",
    "import pandas as pd\n",
    "\n",
    "# Librerias para graficación (trazado de gráficos)\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Necesario para graficar superficies 3D\n",
    "\n",
    "# llama a matplotlib a embeber graficas dentro de los cuadernillos\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar y cargar el DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos el dataset\n",
    "FileDs = r'C:\\Users\\jhean\\OneDrive\\Documentos\\Tareas\\Inteligencia artificial\\DataSets\\workout_fitness_tracker_data.csv'\n",
    "# Cargamos el dataset\n",
    "data = pd.read_csv(FileDs, delimiter=',')\n",
    "data[\"Workout Intensity\"] = data[\"Workout Intensity\"].map({\"Low\": 1, \"Medium\": 2, \"High\": 3})#Convertimos los datos de Workout Intensity a valores numericos\n",
    "X = data.drop(columns=[\"User ID\", \"Calories Burned\", \"Gender\", \"Workout Type\", \"Mood Before Workout\", \"Mood After Workout\", \"Body Fat (%)\", \"VO2 Max\", \"Water Intake (liters)\"]).values  #Filtrado de columnas no relevantes para inferir en y.\n",
    "y = data[\"Calories Burned\"].values  # Variable a inferir en este caso las calorias quemadas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificacion de carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data frame con los datos limpios\n",
    "column_names = [\"Age\", \"Height(cm)\", \"Weight(kg)\", \"Workout Duration(mins)\", \"Heart Rate(bpm)\", \"Steps Taken\", \"Distance(km)\", \"Workout Intensity\", \"Sleep Hours\", \"Daily Calories Intake\", \"Resting Heart Rate(bpm)\"]\n",
    "dataclean = pd.DataFrame(X, columns=column_names)\n",
    "dataclean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar StandardScaler para normalizar los datos\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X) #Este escalador resata la media y divide por la desviacion estandar de cada columna\n",
    "# Las medias y desviaciones estándar (Opcional)\n",
    "#mu = scaler.mean_\n",
    "#sigma = scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame con las caracteristicas normalizados\n",
    "dataclean_normalized = pd.DataFrame(X_normalized, columns=column_names)\n",
    "dataclean_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadir el termino de interseccion o termino independiente\n",
    "$$\n",
    "h_{\\theta}(X) = \\theta_0 + \\theta_1 X_1 + \\theta_2 X_2 + \\dots + \\theta_n X_n\n",
    "$$\n",
    "#Si no añadimos este termino significaria que en caso de que las X son igua a 0 el modelo se veria forzado a pasar por el origen y esto limitaria la capacidad de aprendisaje del modelo y perdiendo precision en el mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_intercept = np.c_[np.ones((X_normalized.shape[0], 1)), X_normalized]  # Con np.c_ concatenamos los arrays y con np.ones añadimos una columna de 1's a X_normalized\n",
    "dataclean_intercept = pd.DataFrame(X_intercept, columns=[\"Intercept\"] + column_names)\n",
    "dataclean_intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desarrollar la Funcion de costo\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "$$\n",
    "h_{\\theta}(X) = \\theta_0 + \\theta_1 X_1 + \\theta_2 X_2 + \\dots + \\theta_n X_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificamos los tipos de arrays\n",
    "print(type(X_intercept))\n",
    "print(type(y))\n",
    "#Conversion de arrays a numpy(En caso de ser neceaario)\n",
    "#xnp = np.array(X_intercept)\n",
    "#ynp = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numero de caracteristicas(incluyendo el termino de sesgo[bias]), recordando que para el nuemero de filas es 0 y para el numero de columnas es 1\n",
    "n = X_intercept.shape[1]\n",
    "#Inicializar theta en ceros ya que si usamos valores grandes o aleatorios el algoritmo de descenso de gradiente puede converger lento o de forma inestable\n",
    "theta = np.zeros(n)\n",
    "#Definimos la hipotesis de la forma simplificada ya que theta esta en 0\n",
    "def hypothesis(X, theta):\n",
    "    return X.dot(theta)#Multiplicacion de matrices\n",
    "#Implementacion de la funcion de costo (MSE)\n",
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = hypothesis(X, theta)\n",
    "    error = h - y\n",
    "    costo = (1/(2*m)) * np.dot(error, error)\n",
    "    #print(\"Costo: \", costo)\n",
    "    return costo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descenso por el gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        h = hypothesis(X, theta)\n",
    "        error = h - y\n",
    "        gradient = (1/m) * np.dot(X.T, error)\n",
    "        theta = theta - alpha * gradient\n",
    "        cost = compute_cost(X, y, theta)\n",
    "        cost_history.append(cost)\n",
    "    return theta, cost_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecucion del algoritmo\n",
    "# Hiperparámetros\n",
    "alpha = 0.01  # Tasa de aprendizaje\n",
    "num_iters = 1000  # Número de iteraciones\n",
    "\n",
    "# Ejecutar el algoritmo\n",
    "theta_final, cost_history = gradient_descent(X_intercept, y, theta, alpha, num_iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Costo mas bajo: 33927.75495975896\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un nuevo ejemplo con valores específicos\n",
    "nuevo_dato = np.array([[39, 175, 99, 79, 112, 8850, 14.44, 3, 8.2, 3195, 61]])\n",
    "dato2= np.array([[56, 154, 89, 39, 118, 14102, 6.55, 2, 5.8, 2071, 65]])\n",
    "#age, altura, peso, duracion del entrenamiento, ritmo cardiaco, pasos dados, distancia, intensidad del entrenamiento, horas de sueño, ingesta diaria de Kcalorias, ritmo cardiaco en reposo\n",
    "\n",
    "# Normalizar los nuevos datos\n",
    "nuevo_dato_normalizado = scaler.transform(nuevo_dato)\n",
    "dato2= scaler.transform(dato2)\n",
    "\n",
    "# Agregar el término de intersección\n",
    "nuevo_dato_intercept = np.c_[np.ones((nuevo_dato_normalizado.shape[0], 1)), nuevo_dato_normalizado]\n",
    "dato2_intercept = np.c_[np.ones((dato2.shape[0], 1)), dato2]\n",
    "\n",
    "# Hacer la predicción con el modelo\n",
    "calorias_predichas = hypothesis(nuevo_dato_intercept, theta_final)\n",
    "calorias_predichas2 = hypothesis(dato2_intercept, theta_final)\n",
    "\n",
    "print(f\" Calorías quemadas estimadas: {calorias_predichas[0]:.2f} kcal\")\n",
    "print(f\" Calorías quemadas estimadas: {calorias_predichas2[0]:.2f} kcal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar presicion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validacion mediante regla de 3 simple\n",
    "X1=(384*100)/553.66\n",
    "X2=100-(118*100/672)\n",
    "# Imprimir resultados\n",
    "print(f\"Precision respecto al primer valor 1: {X1:.2f}%\")\n",
    "print(f\"Presicion respecto al valor 4: {X2:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(num_iters), cost_history, color='blue')\n",
    "plt.title(\"Convergencia del Costo durante el Descenso por Gradiente\")\n",
    "plt.xlabel(\"Número de Iteraciones\")\n",
    "plt.ylabel(\"Costo\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
